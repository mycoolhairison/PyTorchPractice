## Script following Andrej Karpathy's MakeMore Lecture 3.
## Broad goal is to "make more" names given a set of ~30000 names. 
## The goal of Lecture 3 is to give a better understanding of
## the activations, gradients, and their statistics.  Highlights
## include proper initialization of weights and batch normalization.
## In the end we make efforts to "torchify" the code from Lecture 2.

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import random

class Linear:

    def __init__(self, fan_in, fan_out, bias=True):
        self.weight = torch.randn((fan_in,fan_out)) / fan_in**0.5
        self.bias = torch.zeros(fan_out) if bias else None

    def __call__(self, x):
        self.out = x @ self.weight
        if self.bias is not None:
            self.out += self.bias
        return self.out
    
    def parameters(self):
        return [self.weight] + ([] if self.bias is None else [self.bias])
    
class BatchNorm1d:
    def __init__(self, dim, eps=1e-5, momentum=0.1):
        self.eps = eps
        self.momentum=momentum
        self.training=True
        self.gamma = torch.ones(dim)
        self.beta = torch.zeros(dim)
        self.running_mean = torch.zeros(dim)
        self.running_var = torch.ones(dim)

    def __call__(self, x):
        # forward pass
        if self.training:
            xmean = x.mean(0, keepdim=True)
            xvar = x.var(0, keepdim=True, unbiased=True)
        else:
            xmean = self.running_mean
            xvar = self.running_var
        xhat = (x-xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance
        self.out = self.gamma * xhat + self.beta

        if self.training:
            with torch.no_grad():
                self.running_mean = (1- self.momentum) * self.running_mean + self.momentum * xmean
                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar
        
        return self.out
    
class Tanh:
    def __call__(self, x):
        self.out = torch.tanh(x)
        return self.out
    def parameters(self):
        return []
    
## Hyperparameters
emb_dim = 15
num_hidden_nodes = 200
block_size = 3


## Char to int and int to char dicts, we use '`'==chr(96) as a delimiter
stoi = {chr(c):c-96 for c in range(96,123)}
itos = {stoi[k]:k for k in stoi}

## Create training / dev / test sets from name list
words = open('AKL_MakeMore/names.txt', 'r').read().splitlines()
random.shuffle(words)
n1 = int(.8*len(words))
n2 = int(.9*len(words))
Xtr, Ytr, Xdev, Ydev, Xte, Yte = [],[],[],[],[],[]
for i,w in enumerate(words):
    context = [0]*block_size
    for ch in w + '`':
        ix = stoi[ch]
        if i<n1:
            Xtr.append(context)
            Ytr.append(ix)
        elif i<n2:
            Xdev.append(context)
            Ydev.append(ix)
        else:
            Xte.append(context)
            Yte.append(ix)
        context = context[1:] + [ix]
Xtr = torch.tensor(Xtr)
Ytr = torch.tensor(Ytr)
Xdev = torch.tensor(Xdev)
Ydev = torch.tensor(Ydev)
Xte = torch.tensor(Xte)
Yte = torch.tensor(Yte)

## This should probably be obtained from the dataset instead of hardcoded,
## but here we're guaranteed lowercase english + our own special char `
vocab_size = 27

C = torch.randn((vocab_size, emb_dim))
layers = [
    Linear(   emb_dim*block_size, num_hidden_nodes), Tanh(),
    #Linear(     num_hidden_nodes, num_hidden_nodes), Tanh(),
    #Linear(     num_hidden_nodes, num_hidden_nodes), Tanh(),
    #Linear(     num_hidden_nodes, num_hidden_nodes), Tanh(),
    #Linear(     num_hidden_nodes, num_hidden_nodes), Tanh(),
    Linear(     num_hidden_nodes, vocab_size),
]

with torch.no_grad():
    layers[-1].weight *= 0.1
    ## Other layers, I think come from Kaiming init
    for layer in layers[:-1]:
        if isinstance(layer, Linear):
            layer.weight *= 5/3

parameters = [C] + [p for layer in layers for p in layer.parameters()]
for p in parameters:
    p.requires_grad = True


batch_size = 64
max_steps = 200000
lossi = []

for epoch in range(max_steps):

    #create minibatch
    ix = torch.randint(0, Xtr.shape[0], (batch_size,))
    Xb, Yb = Xtr[ix], Ytr[ix]

    #forward pass
    emb = C[Xb] #embeds chars to vectors in dim emb_dim
    x = emb.view(emb.shape[0], -1)
    for layer in layers:
        x = layer(x)
    loss = F.cross_entropy(x, Yb)
    #print(f"Epoch {epoch+1}: Loss {loss.item():.4f}")

    #backward pass
    for p in parameters:
        p.grad = None
    loss.backward()
    lr = .1 if epoch<50000 else .01
    for p in parameters:
        p.data += -lr * p.grad

    if (epoch+1)%5000==0:
        embd = C[Xdev]
        xd = embd.view(embd.shape[0], -1)
        for layer in layers:
            xd = layer(xd)
        lossd = F.cross_entropy(xd, Ydev)
        print(f"Epoch {epoch+1}: DevLoss {lossd.item():.4f}, TrLoss {loss.item():.4f}")
    lossi.append(loss.log10().item())

# plt.figure(figsize=(20,4))
# legends = []
# for i, layer in enumerate(layers[:-1]):
#     if isinstance(layer, Tanh):
#         t = layer.out
#         #print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' (i, layer.__class__.__name__, t.mean(), t.std()))
#         hy, hx = torch.histogram(t, density=True)
#         plt.plot(hx[:-1].detach(), hy.detach())
#         legends.append(f'layer {i} {layer.__class__.__name__}')
# plt.legend(legends)
# plt.title('activation distribution')
# #plt.show()

# plt.figure(figsize=(20,4))
# legends = []
# for i, layer in enumerate(layers[:-1]):
#     if isinstance(layer, Tanh):
#         t = layer.out.grad
#         #print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))
#         hy, hx = torch.histogram(t, density=True)
#         plt.plot(hx[:-1].detach(), hy.detach())
#         legends.append(f'layer {i} {layer.__class__.__name__}')
# plt.legend(legends)
# plt.title('gradient distribution')
# #plt.show()


@torch.no_grad()
def split_loss(split):
    x,y = {
        'train': (Xtr,Ytr),
        'dev': (Xdev,Ydev),
        'test': (Xte,Yte),
    }[split]
    emb = C[x]
    x = emb.view(emb.shape[0], -1)
    for layer in layers:
        x = layer(x)
    loss = F.cross_entropy(x, y)
    print(split, loss.item())

for layer in layers:
    layer.training = False
split_loss('train')
split_loss('dev')
split_loss('test')

## Generating samples!!
for _ in range(20):
    out = []
    context = [0]*block_size
    while True:
        emb = C[torch.tensor([context])]
        x = emb.view(emb.shape[0], -1)
        for layer in layers:
            x = layer(x)
        probs = F.softmax(x, dim = 1)
        ix = torch.multinomial(probs, num_samples=1).item()
        context = context[1:] + [ix]
        if ix==0:
            break
        out.append(itos[ix])
    print("".join(out))